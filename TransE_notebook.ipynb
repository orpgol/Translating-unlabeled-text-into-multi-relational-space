{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from IPython.display import display\n",
    "import util\n",
    "from models import BaseModel, dense_maxnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('saved_triplets.pkl', 'rb') as f:\n",
    "# Pickle the 'data' dictionary using the highest protocol available.\n",
    "    triplets = pickle.load(f)\n",
    "    triplets.rename(columns={0: \"head\", 1: \"rel\", 2: \"tail\"}, inplace=True)\n",
    "    msk = np.random.rand(len(triplets)) < 0.8\n",
    "    train = triplets[msk]\n",
    "    test = val = triplets[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (66, 3)\n",
      "Validation shape: (3, 3)\n",
      "Test shape: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "mask = np.zeros(len(train)).astype(bool)\n",
    "lookup = defaultdict(list)\n",
    "for idx,h,r,t in train.itertuples():\n",
    "    lookup[(h,t)].append(idx) \n",
    "for h,r,t in pd.concat((val,test)).itertuples(index=False):\n",
    "    mask[lookup[(h,t)]] = True\n",
    "    mask[lookup[(t,h)]] = True\n",
    "train = train.loc[~mask]\n",
    "heads, tails = set(train['head']), set(train['tail'])\n",
    "val = val.loc[val['head'].isin(heads) & val['tail'].isin(tails)]\n",
    "test = test.loc[test['head'].isin(heads) & test['tail'].isin(tails)]\n",
    "print('Train shape:', train.shape)\n",
    "print('Validation shape:', val.shape)\n",
    "print('Test shape:', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding false statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation shape: (6, 4)\n",
      "Test shape: (6, 4)\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "combined_df = pd.concat((train, val, test))\n",
    "val = util.create_tf_pairs(val, combined_df, rng)\n",
    "test = util.create_tf_pairs(test, combined_df, rng)\n",
    "print('Validation shape:', val.shape)\n",
    "print('Test shape:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # input and target placeholders\n",
    "    head_input = tf.placeholder(tf.int32, shape=[None])\n",
    "    rel_input = tf.placeholder(tf.int32, shape=[None])\n",
    "    tail_input = tf.placeholder(tf.int32, shape=[None])\n",
    "    target = tf.placeholder(tf.float32, shape=[None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposing the data and categorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 20\n",
    "head_cnt = len(set(train['head']))\n",
    "rel_cnt = len(set(train['rel']))\n",
    "tail_cnt = len(set(train['tail']))\n",
    "    \n",
    "with graph.as_default():\n",
    "    # embedding variables\n",
    "    init_sd = 1.0 / np.sqrt(embedding_size)\n",
    "    head_embedding_vars = tf.Variable(tf.truncated_normal([head_cnt, embedding_size], \n",
    "                                                          stddev=init_sd))\n",
    "    rel_embedding_vars = tf.Variable(tf.truncated_normal([rel_cnt, embedding_size], \n",
    "                                                         stddev=init_sd))\n",
    "    tail_embedding_vars = tf.Variable(tf.truncated_normal([tail_cnt, embedding_size], \n",
    "                                                          stddev=init_sd))\n",
    "    # embedding layer for the (h, r, t) triple being fed in as input\n",
    "    head_embed = tf.nn.embedding_lookup(head_embedding_vars, head_input)\n",
    "    rel_embed = tf.nn.embedding_lookup(rel_embedding_vars, rel_input)\n",
    "    tail_embed = tf.nn.embedding_lookup(tail_embedding_vars, tail_input)\n",
    "    # CP model output\n",
    "    output = tf.reduce_sum(tf.mul(tf.mul(head_embed, rel_embed), tail_embed), 1)\n",
    "    \n",
    "# TensorFlow requires integer indices\n",
    "field_categories = (set(train['head']), set(train['rel']), set(train['tail']))\n",
    "train, train_idx_array = util.make_categorical(train, field_categories)\n",
    "val, val_idx_array = util.make_categorical(val, field_categories)\n",
    "# test, test_idx_array = util.make_categorical(test, field_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization and Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  0, 13],\n",
       "       [32,  0, 13],\n",
       "       [ 8,  0, 38],\n",
       "       [54,  0, 38],\n",
       "       [ 1,  0, 24],\n",
       "       [ 1,  0, 38]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which encodes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>rel</th>\n",
       "      <th>tail</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'kitsch'</td>\n",
       "      <td>11</td>\n",
       "      <td>b'pavlova'</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'conditioned'</td>\n",
       "      <td>11</td>\n",
       "      <td>b'pavlova'</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'remarkably'</td>\n",
       "      <td>11</td>\n",
       "      <td>b'very'</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'bye'</td>\n",
       "      <td>11</td>\n",
       "      <td>b'very'</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'large'</td>\n",
       "      <td>11</td>\n",
       "      <td>b'small'</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b'large'</td>\n",
       "      <td>11</td>\n",
       "      <td>b'very'</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             head rel        tail  label\n",
       "0       b'kitsch'  11  b'pavlova'    1.0\n",
       "1  b'conditioned'  11  b'pavlova'    0.0\n",
       "2   b'remarkably'  11     b'very'    1.0\n",
       "3          b'bye'  11     b'very'    0.0\n",
       "4        b'large'  11    b'small'    1.0\n",
       "5        b'large'  11     b'very'    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from util import ContrastiveTrainingProvider\n",
    "\n",
    "batch_provider = ContrastiveTrainingProvider(train_idx_array, batch_pos_cnt=3, \n",
    "                                             separate_head_tail=True)\n",
    "batch_triples, batch_labels = batch_provider.next_batch()\n",
    "batch_df = pd.DataFrame()\n",
    "batch_df['head'] = pd.Categorical.from_codes(batch_triples[:,0], train['head'].cat.categories)\n",
    "batch_df['rel'] = pd.Categorical.from_codes(batch_triples[:,1], train['rel'].cat.categories)\n",
    "batch_df['tail'] = pd.Categorical.from_codes(batch_triples[:,2], train['tail'].cat.categories)\n",
    "batch_df['label'] = batch_labels\n",
    "display(batch_triples)\n",
    "print('which encodes:')\n",
    "display(batch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'main'</td>\n",
       "      <td>b'principal'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'kitsch'</td>\n",
       "      <td>b'pavlova'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'divi'</td>\n",
       "      <td>b'filius'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'freeware'</td>\n",
       "      <td>b'license'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'kami'</td>\n",
       "      <td>b'shinto'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b'business'</td>\n",
       "      <td>b'corporate'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b'large'</td>\n",
       "      <td>b'small'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b'will'</td>\n",
       "      <td>b'would'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b'dogs'</td>\n",
       "      <td>b'dog'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b'glory'</td>\n",
       "      <td>b'god'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b'bread'</td>\n",
       "      <td>b'wine'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b'find'</td>\n",
       "      <td>b'get'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b'lacks'</td>\n",
       "      <td>b'retain'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>b'amazing'</td>\n",
       "      <td>b'stan'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b'job'</td>\n",
       "      <td>b'salary'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b'synergies'</td>\n",
       "      <td>b'cryoprotectant'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>b'goto'</td>\n",
       "      <td>b'nizkor'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>b'thoughtful'</td>\n",
       "      <td>b'harmonious'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>b'alopex'</td>\n",
       "      <td>b'lagopus'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>b'endian'</td>\n",
       "      <td>b'tamarin'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>b'lacks'</td>\n",
       "      <td>b'retain'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>b'nihonshoki'</td>\n",
       "      <td>b'kojiki'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>b'hungry'</td>\n",
       "      <td>b'oed'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>b'image'</td>\n",
       "      <td>b'jpg'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>b'free'</td>\n",
       "      <td>b'open'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>b'usually'</td>\n",
       "      <td>b'typically'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>b'regard'</td>\n",
       "      <td>b'agree'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>b'lovely'</td>\n",
       "      <td>b'mildred'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>b'lycomedes'</td>\n",
       "      <td>b'noblewoman'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>b'jovanovich'</td>\n",
       "      <td>b'harcourt'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>b'anois'</td>\n",
       "      <td>b'libh'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>b'someone'</td>\n",
       "      <td>b'person'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>b'traditional'</td>\n",
       "      <td>b'typical'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>b'magic'</td>\n",
       "      <td>b'spells'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>b'hard'</td>\n",
       "      <td>b'soft'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>b'careful'</td>\n",
       "      <td>b'exacting'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>b'conditioned'</td>\n",
       "      <td>b'venn'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>b'those'</td>\n",
       "      <td>b'them'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>b'mutual'</td>\n",
       "      <td>b'friendship'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>b'ciao'</td>\n",
       "      <td>b'edie'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>b'bye'</td>\n",
       "      <td>b'thank'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>b'dim'</td>\n",
       "      <td>b'pies'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>b'pona'</td>\n",
       "      <td>b'ela'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>b'bye'</td>\n",
       "      <td>b'thank'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>b'successful'</td>\n",
       "      <td>b'unsuccessful'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>b'consulting'</td>\n",
       "      <td>b'firm'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>b'nihonshoki'</td>\n",
       "      <td>b'kojiki'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>b'baggini'</td>\n",
       "      <td>b'cesarani'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>b'tempelhof'</td>\n",
       "      <td>b'aschaffenburg'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>b'thank'</td>\n",
       "      <td>b'please'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>b'remarkably'</td>\n",
       "      <td>b'very'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>b'lot'</td>\n",
       "      <td>b'lots'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>b'quickly'</td>\n",
       "      <td>b'eventually'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>b'estimation'</td>\n",
       "      <td>b'nizkor'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>b'masa'</td>\n",
       "      <td>b'tortillas'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>b'proud'</td>\n",
       "      <td>b'happy'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>b'prudent'</td>\n",
       "      <td>b'iptables'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>b'breed'</td>\n",
       "      <td>b'breeds'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>b'lycomedes'</td>\n",
       "      <td>b'noblewoman'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>b'yam'</td>\n",
       "      <td>b'candied'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              head               tail\n",
       "0          b'main'       b'principal'\n",
       "1        b'kitsch'         b'pavlova'\n",
       "2          b'divi'          b'filius'\n",
       "3      b'freeware'         b'license'\n",
       "4          b'kami'          b'shinto'\n",
       "5      b'business'       b'corporate'\n",
       "6         b'large'           b'small'\n",
       "7          b'will'           b'would'\n",
       "8          b'dogs'             b'dog'\n",
       "9         b'glory'             b'god'\n",
       "10        b'bread'            b'wine'\n",
       "11         b'find'             b'get'\n",
       "12        b'lacks'          b'retain'\n",
       "13      b'amazing'            b'stan'\n",
       "14          b'job'          b'salary'\n",
       "15    b'synergies'  b'cryoprotectant'\n",
       "16         b'goto'          b'nizkor'\n",
       "17   b'thoughtful'      b'harmonious'\n",
       "18       b'alopex'         b'lagopus'\n",
       "19       b'endian'         b'tamarin'\n",
       "20        b'lacks'          b'retain'\n",
       "21   b'nihonshoki'          b'kojiki'\n",
       "22       b'hungry'             b'oed'\n",
       "23        b'image'             b'jpg'\n",
       "24         b'free'            b'open'\n",
       "25      b'usually'       b'typically'\n",
       "26       b'regard'           b'agree'\n",
       "27       b'lovely'         b'mildred'\n",
       "28    b'lycomedes'      b'noblewoman'\n",
       "29   b'jovanovich'        b'harcourt'\n",
       "..             ...                ...\n",
       "36        b'anois'            b'libh'\n",
       "37      b'someone'          b'person'\n",
       "38  b'traditional'         b'typical'\n",
       "39        b'magic'          b'spells'\n",
       "40         b'hard'            b'soft'\n",
       "41      b'careful'        b'exacting'\n",
       "42  b'conditioned'            b'venn'\n",
       "43        b'those'            b'them'\n",
       "44       b'mutual'      b'friendship'\n",
       "45         b'ciao'            b'edie'\n",
       "46          b'bye'           b'thank'\n",
       "47          b'dim'            b'pies'\n",
       "48         b'pona'             b'ela'\n",
       "49          b'bye'           b'thank'\n",
       "50   b'successful'    b'unsuccessful'\n",
       "51   b'consulting'            b'firm'\n",
       "52   b'nihonshoki'          b'kojiki'\n",
       "53      b'baggini'        b'cesarani'\n",
       "54    b'tempelhof'   b'aschaffenburg'\n",
       "55        b'thank'          b'please'\n",
       "56   b'remarkably'            b'very'\n",
       "57          b'lot'            b'lots'\n",
       "58      b'quickly'      b'eventually'\n",
       "59   b'estimation'          b'nizkor'\n",
       "60         b'masa'       b'tortillas'\n",
       "61        b'proud'           b'happy'\n",
       "62      b'prudent'        b'iptables'\n",
       "63        b'breed'          b'breeds'\n",
       "64    b'lycomedes'      b'noblewoman'\n",
       "65          b'yam'         b'candied'\n",
       "\n",
       "[66 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['head','tail']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining optional loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares_objective(output, target, add_bias=True):\n",
    "    y = output\n",
    "    if add_bias:\n",
    "        bias = tf.Variable([0.0])\n",
    "        y = output + bias\n",
    "    loss = tf.reduce_sum(tf.square(y - target))\n",
    "    return y, loss\n",
    "\n",
    "def logistic_objective(output, target, add_bias=True):\n",
    "    y = output\n",
    "    if add_bias:\n",
    "        bias = tf.Variable([0.0])\n",
    "        y = output + bias\n",
    "    squashed_y = tf.clip_by_value(tf.sigmoid(y), 0.001, 0.999) # avoid NaNs\n",
    "    loss = -tf.reduce_sum(target*tf.log(squashed_y) + (1-target)*tf.log(1-squashed_y))\n",
    "    return squashed_y, loss\n",
    "\n",
    "def ranking_margin_objective(output, margin=1.0):\n",
    "    y_pairs = tf.reshape(output, [-1,2]) # fold: 1 x n -> [n/2 x 2]\n",
    "    pos_scores, neg_scores = tf.split(1, 2, y_pairs) # separate the pairs\n",
    "    hinge_losses = tf.nn.relu(margin - pos_scores + neg_scores)\n",
    "    total_hinge_loss = tf.reduce_sum(hinge_losses)\n",
    "    return output, total_hinge_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransE(BaseModel):\n",
    "\n",
    "    def __init__(self, embedding_size, batch_pos_cnt=100, \n",
    "                 max_iter=1000, dist='euclidean', \n",
    "                 margin=1.0, opt=None):\n",
    "        super(TransE, self).__init__(embedding_size=embedding_size,\n",
    "                                     maxnorm=1.0,\n",
    "                                     batch_pos_cnt=batch_pos_cnt,\n",
    "                                     max_iter=max_iter,\n",
    "                                     model_type='ranking_margin',\n",
    "                                     opt=opt)\n",
    "        self.dist = dist\n",
    "        self.margin = margin\n",
    "        self.EPS = 1e-3 # for sqrt gradient when dist='euclidean'\n",
    "    \n",
    "    def _create_model(self, train_triples):\n",
    "        # Count unique items to determine embedding matrix sizes\n",
    "        entity_cnt = len(set(train_triples[:,0]).union(train_triples[:,2]))\n",
    "        rel_cnt = len(set(train_triples[:,1]))\n",
    "        init_sd = 1.0 / np.sqrt(self.embedding_size)\n",
    "        # Embedding variables\n",
    "        entity_var_shape = [entity_cnt, self.embedding_size]\n",
    "        rel_var_shape = [rel_cnt, self.embedding_size]\n",
    "        entity_init  = tf.truncated_normal(entity_var_shape, stddev=init_sd)\n",
    "        rel_init = tf.truncated_normal(rel_var_shape, stddev=init_sd)\n",
    "        # Ensure maxnorm constraints are initially satisfied\n",
    "        entity_init = dense_maxnorm(entity_init, self.maxnorm)\n",
    "        self.entity_embedding_vars = tf.Variable(entity_init)\n",
    "        self.rel_embedding_vars = tf.Variable(rel_init)\n",
    "        # Embedding layer for each (head, rel, tail) triple being fed in as input\n",
    "        head_embed = tf.nn.embedding_lookup(self.entity_embedding_vars, self.head_input)\n",
    "        tail_embed = tf.nn.embedding_lookup(self.entity_embedding_vars, self.tail_input)\n",
    "        rel_embed = tf.nn.embedding_lookup(self.rel_embedding_vars, self.rel_input)\n",
    "        # Relationship vector acts as a translation in entity embedding space\n",
    "        diff_vec = tail_embed - (head_embed + rel_embed)\n",
    "        # negative dist so higher scores are better (important for pairwise loss)\n",
    "        if self.dist == 'manhattan':\n",
    "            raw_output = -tf.reduce_sum(tf.abs(diff_vec), 1)\n",
    "        elif self.dist == 'euclidean':\n",
    "            # +eps because gradients can misbehave for small values in sqrt\n",
    "            raw_output = -tf.sqrt(tf.reduce_sum(tf.square(diff_vec), 1) + self.EPS)\n",
    "        elif self.dist == 'sqeuclidean':\n",
    "            raw_output = -tf.reduce_sum(tf.square(diff_vec), 1)\n",
    "        else:\n",
    "            raise Exception('Unknown distance type')\n",
    "        # Model output\n",
    "        self.output, self.loss = ranking_margin_objective(raw_output, self.margin)\n",
    "        # Optimization with postprocessing to limit embedding vars to L2 ball\n",
    "        self.train_step = self.opt.minimize(self.loss)\n",
    "        unique_ent_indices = tf.unique(tf.concat(0, [self.head_input, self.tail_input]))[0]\n",
    "        self.post_step = self._norm_constraint_op(self.entity_embedding_vars, \n",
    "                                                  unique_ent_indices, \n",
    "                                                  self.maxnorm)\n",
    "# feed dict for monitoring progress on validation set\n",
    "val_labels = np.array(val['truth_flag'], dtype=np.float)\n",
    "val_feed_dict = {head_input: val_idx_array[:,0],\n",
    "                 rel_input: val_idx_array[:,1],\n",
    "                 tail_input: val_idx_array[:,2],\n",
    "                 target: val_labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning step and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transE = TransE(embedding_size=20,\n",
    "                margin=1.0,\n",
    "                dist='euclidean',\n",
    "                batch_pos_cnt=100, \n",
    "                max_iter=30000)\n",
    "\n",
    "val_feed_dict = transE.create_feed_dict(val_idx_array)\n",
    "\n",
    "def train_step_callback(itr, batch_feed_dict):\n",
    "    if (itr % 2000) == 0 or (itr == (transE.max_iter-1)):\n",
    "        batch_size = len(batch_feed_dict[transE.target])\n",
    "        batch_avg_loss = transE.sess.run(transE.loss, batch_feed_dict) / batch_size\n",
    "        val_output, val_loss = transE.sess.run((transE.output, transE.loss), val_feed_dict)\n",
    "        val_avg_loss = val_loss / len(val_labels)\n",
    "        val_pair_ranking_acc = util.pair_ranking_accuracy(val_output)\n",
    "        msg = 'itr {}, batch loss: {:.2}, val loss: {:.2}, val pair ranking acc: {:.2}'\n",
    "        print(msg.format(itr, batch_avg_loss, val_avg_loss, val_pair_ranking_acc))\n",
    "    return True\n",
    "\n",
    "transE.fit(train_idx_array, train_step_callback)\n",
    "\n",
    "acc, pred, scores, thresh_map = util.model_threshold_and_eval(transE, test, val)\n",
    "print('Test set accuracy: {:.2}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, output_notebook, show, ColumnDataSource, reset_output\n",
    "from bokeh.palettes import Spectral11\n",
    "\n",
    "subsample = 10\n",
    "\n",
    "reset_output()\n",
    "output_notebook()\n",
    "\n",
    "lexnames = pd.read_table(os.path.join(data_dir, 'wordnet_lexnames.txt'), index_col=0)\n",
    "entity_embeddings = transE.sess.run(transE.entity_embedding_vars)\n",
    "entity_cats = train['head'].cat.categories\n",
    "entity_names = pd.Categorical.from_codes(range(len(entity_embeddings)), \n",
    "                                         entity_cats).astype(str)\n",
    "entity_lexnames = lexnames.loc[entity_names].values\n",
    "\n",
    "# Run on just a subset of the data to save some time\n",
    "emb_subset = entity_embeddings[::subsample, :] \n",
    "emb_subset_names = entity_names[::subsample]\n",
    "emb_subset_lexnames = entity_lexnames[::subsample]\n",
    "\n",
    "print('Embeddings shape:', entity_embeddings.shape)\n",
    "print('Using subset:', emb_subset.shape)\n",
    "print('Running T-SNE, may take a while...')\n",
    "tsne = TSNE(n_iter=1000, method='barnes_hut')\n",
    "lowdim = tsne.fit_transform(emb_subset)\n",
    "\n",
    "print('Plotting...')\n",
    "source = ColumnDataSource(\n",
    "  data=dict(x=lowdim[:,0],\n",
    "            y=lowdim[:,1],\n",
    "            name=emb_subset_names,\n",
    "            lexname=emb_subset_lexnames)\n",
    ")\n",
    "colormap = {}\n",
    "for i,ln in enumerate(set(emb_subset_lexnames.flat)):\n",
    "    colormap[ln] = Spectral11[i % len(Spectral11)]\n",
    "colors = [colormap[ln] for ln in emb_subset_lexnames.flat]\n",
    "tools = 'pan,wheel_zoom,box_zoom,reset,resize,hover'\n",
    "fig = figure(title=\"T-SNE of WordNet TransE Embeddings\", \n",
    "             plot_width=800, plot_height=600, tools=tools)\n",
    "fig.scatter('x', 'y', source=source, alpha=0.5, fill_color=colors, line_color=None)\n",
    "hover = fig.select(dict(type=HoverTool))\n",
    "hover.tooltips = [('','@name, @lexname')]\n",
    "h = show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
